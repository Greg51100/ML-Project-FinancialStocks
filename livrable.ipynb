{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d406fd89",
   "metadata": {},
   "source": [
    "# US Stock Market Prediction (2014-2018)\n",
    "**Authors:** David PAGNIEZ, Antoine KRYCHOWSKI, Gregoire MEHAH\n",
    "\n",
    "## 1. Project Objective\n",
    "The goal of this project is to analyze financial indicators of US companies from 2014 to 2018 and build a machine learning model to predict stock performance.\n",
    "\n",
    "Our approach follows these steps:\n",
    "1.  **Data Cleaning:** Handling missing values and outliers in financial data.\n",
    "2.  **Baseline Model:** Attempting to predict the exact price variation (Regression).\n",
    "3.  **Classification:** Switching to a trend prediction (Buy/Sell) if regression fails.\n",
    "4.  **Optimization:** Using Feature Engineering and Ensemble methods to improve results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f77608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 2014_Financial_Data.csv\n",
      "Loaded 2015_Financial_Data.csv\n",
      "Loaded 2016_Financial_Data.csv\n",
      "Loaded 2017_Financial_Data.csv\n",
      "Loaded 2018_Financial_Data.csv\n",
      "Initial Dataset Shape: (22077, 225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoi\\AppData\\Local\\Temp\\ipykernel_24232\\3272686016.py:34: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['Target_Price_Var'] = data[target_cols].bfill(axis=1).iloc[:, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "# Visual configuration\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "# 1. Loading Data\n",
    "# We iterate through the CSV files to merge them into a single dataframe.\n",
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "dfs = []\n",
    "\n",
    "print(\"Loading data...\")\n",
    "for year in years:\n",
    "    try:\n",
    "        filename = f'{year}_Financial_Data.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "        df['Year'] = year\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found.\")\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 2. Target Consolidation\n",
    "# The target column name is not consistent across files, so we merge them.\n",
    "target_cols = [col for col in data.columns if 'PRICE VAR' in col]\n",
    "data['Target_Price_Var'] = data[target_cols].bfill(axis=1).iloc[:, 0]\n",
    "data['Target_Price_Var'] = pd.to_numeric(data['Target_Price_Var'], errors='coerce')\n",
    "\n",
    "# Dropping useless columns\n",
    "cols_to_drop = target_cols + ['Class', 'Unnamed: 0', 'Symbol']\n",
    "data = data.drop(columns=[c for c in cols_to_drop if c in data.columns])\n",
    "\n",
    "print(f\"Initial Dataset Shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ee178",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and Outlier Management\n",
    "\n",
    "Financial data is known to be very noisy. Some companies are massive (Apple, Google), while others are small. Additionally, some data points seem to be errors (e.g., a 10,000% increase).\n",
    "\n",
    "If we leave these outliers, they will bias the model.\n",
    "Strategy: We apply Winsorization (Clipping). We cap the values at the 1st and 99th percentiles to remove extreme outliers without deleting too much data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9d2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done. Outliers have been clipped.\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove rows where the target is missing\n",
    "data_clean = data.dropna(subset=['Target_Price_Var']).copy()\n",
    "\n",
    "# 2. Remove columns with too many missing values (> 40%)\n",
    "threshold = 0.4\n",
    "data_clean = data_clean.dropna(thresh=len(data_clean) * (1 - threshold), axis=1)\n",
    "\n",
    "# 3. Winsorization (Clipping)\n",
    "# We limit the Target between -99% (almost bankruptcy) and +500% (x5)\n",
    "data_clean['Target_Price_Var'] = data_clean['Target_Price_Var'].clip(lower=-99, upper=500)\n",
    "\n",
    "# We limit the Features between the 1st and 99th quantile\n",
    "numeric_cols = data_clean.select_dtypes(include=np.number).columns.drop(['Year', 'Target_Price_Var'])\n",
    "lower_bounds = data_clean[numeric_cols].quantile(0.01)\n",
    "upper_bounds = data_clean[numeric_cols].quantile(0.99)\n",
    "\n",
    "data_clean[numeric_cols] = data_clean[numeric_cols].clip(lower=lower_bounds, upper=upper_bounds, axis=1)\n",
    "\n",
    "print(\"Preprocessing done. Outliers have been clipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fc3ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
